version: '3.8' # Specify Compose file version

services:
  # Service 1: Ollama with Intel GPU support
  ollama:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ipex-llm-ollama # Static name instead of timestamp
    devices:
      - /dev/dri:/dev/dri # Map the GPU device
    ports:
      - "11434:11434" # Expose port 11434 on the host
    volumes:
      # Use ${HOME} for better compatibility than ~
      - ${HOME}/.ollama/models:/root/.ollama/models
    environment:
      # Use map syntax for environment variables (easier to read)
      PATH: /llm/ollama:${PATH}
      OLLAMA_HOST: 0.0.0.0
      no_proxy: localhost,127.0.0.1
      ZES_ENABLE_SYSMAN: 1
      OLLAMA_INTEL_GPU: 'true' # Quote boolean/numeric strings if needed
      OLLAMA_NUM_GPU: 999
      OLLAMA_KV_CACHE_TYPE: q8_0
      OLLAMA_FLASH_ATTENTION: 1
      SYCL_CACHE_PERSISTENT: 1
      ONEAPI_DEVICE_SELECTOR: level_zero:0
      DEVICE: Arc
    shm_size: '32g' # Shared memory size
    mem_limit: '32G' # Memory limit (alternative to deploy.resources)
    command: >
      bash -c "
      cd /llm/scripts/ && \
      source ipex-llm-init --gpu --device Arc && \
      bash start-ollama.sh && \
      tail -f /llm/ollama/ollama.log
      "
    restart: unless-stopped # Optional: restart policy

  # Service 2: Open WebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    network_mode: host
    ports:
      - "8080:8080" # Default WebUI port
    volumes:
      # It now maps a host directory to the container's data directory.
      - ${HOME}/.local/share/open_webui:/app/backend/data
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
    depends_on:
      - ollama # Ensures Ollama starts before WebUI (optional but good practice)
    restart: always

# This will repopulate freshly inside .local/share/open_webui
# docker compose up -d --force-recreate
