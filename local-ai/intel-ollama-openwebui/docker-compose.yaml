version: '3.8' # Specify Compose file version

services:
  # Service 1: Ollama with Intel GPU support
  ollama:
    image: intelanalytics/ipex-llm-inference-cpp-xpu:latest
    container_name: ipex-llm-ollama # Static name instead of timestamp
    # network_mode: bridge # Replicates --net=bridge. See notes below.
    devices:
      - /dev/dri:/dev/dri # Map the GPU device
    ports:
      - "11434:11434" # Expose port 11434 on the host
    volumes:
      # Use ${HOME} for better compatibility than ~
      - ${HOME}/.ollama/models:/root/.ollama/models
    environment:
      # Use map syntax for environment variables (easier to read)
      PATH: /llm/ollama:${PATH}
      OLLAMA_HOST: 0.0.0.0
      no_proxy: localhost,127.0.0.1
      ZES_ENABLE_SYSMAN: 1
      OLLAMA_INTEL_GPU: 'true' # Quote boolean/numeric strings if needed
      OLLAMA_NUM_GPU: 999
      OLLAMA_KV_CACHE_TYPE: q8_0
      OLLAMA_FLASH_ATTENTION: 1
      SYCL_CACHE_PERSISTENT: 1
      ONEAPI_DEVICE_SELECTOR: level_zero:0
      DEVICE: Arc
    shm_size: '16g' # Shared memory size
    mem_limit: '32G' # Memory limit (alternative to deploy.resources)
    # The command to run inside the container
    # Note: YAML handles multi-line strings well
    command: >
      bash -c "
      cd /llm/scripts/ && \
      source ipex-llm-init --gpu --device Arc && \
      bash start-ollama.sh && \
      tail -f /llm/ollama/ollama.log
      "
    restart: unless-stopped # Optional: restart policy

  # Service 2: Open WebUI
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    # network_mode: host # Replicates --network=host. See notes below.
    ports:
      # If not using network_mode: host, you need to expose the WebUI port
      # For host mode, this isn't strictly needed but doesn't hurt
      - "8080:8080" # Default WebUI port, adjust if you changed it
    volumes:
      - open-webui-data:/app/backend/data # Use a named volume
    environment:
      # IMPORTANT: How WebUI finds Ollama depends on networking (see notes)
      # Option 1 (If Ollama is on bridge/default & WebUI is on host):
      # OLLAMA_BASE_URL: http://127.0.0.1:11434
      # Option 2 (If BOTH are on the default Compose network - RECOMMENDED):
      OLLAMA_BASE_URL: http://ollama:11434
    depends_on:
      - ollama # Ensures Ollama starts before WebUI (optional but good practice)
    restart: always

# Define the named volume used by open-webui
volumes:
  open-webui-data:

# Define networks (Optional, needed if you want more control than the default)
# networks:
#  default:
#    driver: bridge
